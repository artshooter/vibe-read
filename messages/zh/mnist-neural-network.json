{
  "metadata": {
    "title": "MNIST - 从零手撸神经网络",
    "description": "理解手写数字识别背后的原理：卷积层、激活函数、全连接层如何协同工作，以及如何通过梯度下降训练模型"
  },
  "hero": {
    "loginBanner": "Last login: Sat Dec 28 2025 on ttys001",
    "subtitle": "从零手撸神经网络",
    "initializing": "正在初始化教程...",
    "ready": "准备就绪。",
    "helpText": "向下滚动开始学习神经网络的工作原理",
    "scrollHint": "向下滚动",
    "clickToRead": "点击阅读 →"
  },
  "sections": {
    "overview": {
      "title": "MNIST - 机器学习的 Hello World",
      "intro": "MNIST 手写数字识别问题被视作机器学习领域的 \"Hello, World\"。我们将从零开始手撸一个神经网络（不使用深度学习框架），来解决这个经典问题。该模型包含：卷积层、激活函数层、全连接层。",
      "flowTitle": "数据处理流程",
      "flow": {
        "step1": "卷积层：通过矩阵计算扫描数据，提取特征",
        "step2": "激活函数层：引入非线性变换，进一步提取特征",
        "step3": "全连接层：对所有数据进行加权求和，输出结果",
        "step4": "训练优化：通过损失函数和梯度下降不断调整参数"
      }
    },
    "convolution": {
      "title": "卷积层 - 如何提取特征",
      "intro": "卷积层是神经网络的核心组件之一。它通过矩阵计算来扫描输入数据，从而提取出有用的特征。想象一下用一个小窗口在图片上滑动，每次计算窗口内的数值，这就是卷积的基本原理。",
      "visualizer": {
        "title": "交互式卷积计算演示"
      },
      "howItWorks": {
        "title": "工作原理",
        "step1": "卷积核（Kernel）在输入矩阵上滑动，每次覆盖一个 3×3 的区域",
        "step2": "将卷积核与对应位置的输入值逐个相乘，然后求和，得到一个输出值",
        "step3": "不同的卷积核可以提取不同的特征。例如，横向边缘检测器能够识别图像中的横向笔画"
      }
    },
    "activation": {
      "title": "激活函数层 - 引入非线性",
      "intro": "激活函数的作用是引入非线性变换，使神经网络能够学习复杂的模式。ReLU（Rectified Linear Unit）是最常用的激活函数之一，它将所有负数变为 0，保留正数不变。",
      "explanation": "通过 ReLU 激活函数，我们抑制了负值（可能是噪声），保留了正值（可能是有用的特征）。这种选择性的信息过滤，让神经网络能够更好地识别复杂模式。"
    },
    "fullyConnected": {
      "title": "全连接层 - 输出预测结果",
      "intro": "全连接层将前面提取的所有特征进行加权求和，最终输出对每个数字（0-9）的预测得分。得分最高的数字就是模型的预测结果。",
      "explanation": "每个数字都有自己专属的权重组。模型通过学习，让正确数字的权重组能够产生更高的得分。在这个例子中，数字 7 的得分是 0.91，远高于其他数字，所以模型预测这个图像是数字 7。"
    },
    "training": {
      "title": "模型训练 - 如何学习",
      "intro": "训练模型本质上是一个反馈调节的过程。通过不断计算、评估、调整参数，模型逐步提升识别准确率。",
      "lossFunction": {
        "title": "损失函数 - 评估预测质量",
        "description": "损失函数用来衡量模型预测结果与真实结果之间的差距。差距越大，损失值越高。我们的目标是通过训练让损失值不断降低。"
      },
      "gradient": {
        "title": "梯度 - 寻找优化方向",
        "description": "通过求导数（梯度），我们可以知道该如何调整模型参数才能降低损失。梯度指向损失增加最快的方向，我们反方向调整参数，就能让损失下降。"
      },
      "optimization": {
        "title": "优化器 - 更新参数",
        "description": "优化器根据梯度来更新模型的权重参数。每次训练（epoch）后，权重都会朝着降低损失的方向调整一点点。经过多轮训练，模型的准确率不断提升。"
      }
    },
    "summary": {
      "title": "总结",
      "content": "我们从零实现了一个手写数字识别的神经网络。通过卷积层提取特征，激活函数引入非线性，全连接层输出预测，最后用梯度下降训练模型。虽然这只是一个简单的网络，但它包含了深度学习的核心思想：用数据驱动的方式，让机器自动学习特征和模式。"
    }
  },
  "details": {
    "convolution": {
      "command": "cat convolution_math.txt",
      "title": "卷积层详细计算过程",
      "intro": "让我们用一个具体例子来理解卷积计算的每一步。",
      "matrices": {
        "input": "图像（数字7）的数据矩阵",
        "kernel": "权重矩阵（横向特征检测器）",
        "output": "特征矩阵"
      },
      "steps": {
        "step1": "第一步：卷积核覆盖输入矩阵的左上角 3×3 区域",
        "step2": "将对应位置的元素相乘：(1×1) + (1×1) + (1×1) + (0×0) + (0×0) + (1×0) + (0×-1) + (0×-1) + (1×-1) = 3 - 1 = 2",
        "step3": "卷积核向右滑动一格，重复上述计算",
        "step4": "当到达右边界后，卷积核向下移动一行，从左边开始继续",
        "step5": "遍历完所有位置后，得到完整的特征矩阵"
      },
      "calculation": "计算第一个位置：\\n\\n1×1 + 1×1 + 1×1 = 3 (第一行)\\n0×0 + 0×0 + 0×0 = 0 (第二行)\\n0×(-1) + 0×(-1) + 1×(-1) = -1 (第三行)\\n\\n总和：3 + 0 - 1 = 2",
      "insight": "横向特征检测器的工作原理：\\n• 当遇到横向笔画时（如数字7的顶部），输出较大的正值\\n• 当遇到纵向笔画时（如数字1），输出较小的值\\n• 通过这种方式，不同的卷积核可以提取不同方向的特征",
      "comparison": "对比结果：\\n数字7的特征矩阵：第一行有较大的值（3, 2, 1），因为顶部有横向笔画\\n数字1的特征矩阵：所有值都较小且均匀（全为1），因为主要是纵向笔画"
    },
    "activation": {
      "command": "cat activation_function.txt",
      "title": "激活函数详细说明",
      "intro": "ReLU（Rectified Linear Unit）是最常用的激活函数，公式为：f(x) = max(0, x)",
      "example": "输入矩阵 A：\\n[  3   2   1  ]\\n[  1   1   1  ]\\n[ -1  -1  -1 ]\\n\\n应用 ReLU 后：\\n[  3   2   1  ]\\n[  1   1   1  ]\\n[  0   0   0  ]",
      "explanation": "为什么要用激活函数？",
      "reasons": {
        "linearity": "线性变换的局限：如果神经网络只做矩阵乘法（线性变换），无论有多少层，最终效果都等同于一个矩阵乘法。这样的网络无法学习复杂的非线性模式。",
        "nonlinearity": "引入非线性：激活函数让神经网络能够学习复杂的、非线性的关系。例如，图像中的边缘、形状等特征往往不是简单的线性组合。",
        "selectivity": "选择性增强：ReLU 会抑制负值（可能是噪声或无关特征），保留正值（可能是有用的特征），从而提高模型的表达能力。"
      },
      "otherFunctions": "常见激活函数：\\n• Sigmoid: f(x) = 1 / (1 + e^(-x)) - 输出范围在 (0,1)\\n• Tanh: f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) - 输出范围在 (-1,1)\\n• ReLU: f(x) = max(0, x) - 最常用，计算简单\\n• Leaky ReLU: f(x) = max(0.01x, x) - 解决 ReLU 的「死亡神经元」问题"
    },
    "fullyConnected": {
      "command": "cat fully_connected_math.txt",
      "title": "全连接层详细推导",
      "intro": "全连接层将特征矩阵展平后，与权重矩阵相乘，得到每个数字的得分。",
      "process": {
        "flatten": "第一步：展平特征矩阵\\n\\n输入：3×3 的特征矩阵\\n[ 3  2  1 ]\\n[ 1  1  1 ]\\n[ 0  0  0 ]\\n\\n展平后：[3, 2, 1, 1, 1, 1, 0, 0, 0]",
        "weights": "第二步：准备权重矩阵\\n\\n每个数字（0-9）都有自己的权重向量\\n例如数字 0 的权重：[0.01, 0.01, 0.01, 0.01, 0.02, 0, 0, 0, 0.02]\\n例如数字 7 的权重：[0.15, 0.12, 0.08, 0.03, 0.03, 0.03, 0, 0, 0]",
        "calculation": "第三步：计算得分\\n\\n数字 0 的得分：\\n3×0.01 + 2×0.01 + 1×0.01 + 1×0.01 + 1×0.02 + 1×0 + 0×0 + 0×0 + 0×0.02\\n= 0.03 + 0.02 + 0.01 + 0.01 + 0.02\\n= 0.09\\n\\n数字 7 的得分：\\n3×0.15 + 2×0.12 + 1×0.08 + 1×0.03 + 1×0.03 + 1×0.03 + 0×0 + 0×0 + 0×0\\n= 0.45 + 0.24 + 0.08 + 0.03 + 0.03 + 0.03\\n= 0.86"
      },
      "output": "最终输出（10个数字的得分）：\\n[0.09, 0.02, 0.05, 0.09, 0.12, 0.04, 0.05, 0.86, 0.06, 0.08]\\n        ↑\\n   数字7得分最高",
      "insight": "关键理解：\\n• 每个输出维度（数字）独立计算\\n• 权重是训练过程中学习得到的\\n• 得分高低反映了模型对该数字的「信心」\\n• 最终预测 = argmax(得分向量)"
    },
    "training": {
      "command": "cat training_process.txt",
      "title": "训练过程详细推导",
      "lossFunction": {
        "title": "损失函数（均方差）",
        "formula": "Loss = Σ(T_i - P_i)²",
        "example": "预测结果 P = [0.09, 0.02, 0.05, 0.09, 0.12, 0.04, 0.05, 0.91, 0.06, 0.08]\\n真实标签 T = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\\n\\n损失计算：\\n(0-0.09)² + (0-0.02)² + ... + (1-0.91)² + ... + (0-0.08)²\\n= 0.0081 + 0.0004 + ... + 0.0081 + ... + 0.0064\\n≈ 0.0523",
        "meaning": "损失值越小，说明预测越准确。我们的目标是通过训练让损失值不断减小。"
      },
      "gradient": {
        "title": "梯度计算",
        "intro": "梯度告诉我们如何调整参数才能降低损失。",
        "calculation": "初始梯度（从损失函数对输出求导）：\\ngradient = P - T\\n= [0.09, 0.02, 0.05, 0.09, 0.12, 0.04, 0.05, 0.91, 0.06, 0.08] - [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\\n= [0.09, 0.02, 0.05, 0.09, 0.12, 0.04, 0.05, -0.09, 0.06, 0.08]\\n\\n注意：数字7的梯度是负数（-0.09），说明我们需要增大这个权重",
        "backprop": "反向传播：\\n1. 从输出层开始，计算梯度\\n2. 利用链式法则，将梯度传递到前一层\\n3. 每一层都更新自己的权重\\n4. 一直传播到第一层"
      },
      "optimization": {
        "title": "权重更新（梯度下降）",
        "formula": "新权重 = 旧权重 - 学习率 × 梯度",
        "example": "假设学习率 = 0.01\\n\\n全连接层权重更新：\\nW_new(i,j) = W_old(i,j) - 0.01 × gradient[i] × input[j]\\n\\n例如，数字0的第一个权重：\\nW_new(0,0) = W_old(0,0) - 0.01 × 0.09 × 3\\n         = W_old(0,0) - 0.0027",
        "iterations": "训练迭代：\\n• 前向传播：输入 → 卷积 → 激活 → 全连接 → 输出\\n• 计算损失：对比预测与真实标签\\n• 反向传播：计算每层的梯度\\n• 更新权重：根据梯度调整参数\\n• 重复以上步骤数千或数万次"
      }
    }
  }
}
